{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Scikit-learn: Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEK 4: Supervised Learning: Explaining Titanic Hypothesis with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The problem we would like to solve is to determine if a Titanic's passenger would have survived, given her age, passenger class, and sex._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing numpy, scikit-learn, and pyplot, the Python libraries we will be using here. Show the versions we will be using (in case you have problems running the notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = '/data/MLSem/Ex4/'\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "print ('matplotlib version:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we should first load the dataset. The list of attributes it includes is: Ordinal, Class, Survived (0=no, 1=yes), Name, Age, Port of Embarkation, Home/Destination, Room, Ticket, Boat, and Sex. We will start by loading the dataset into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(DATA_PATH + 'titanic.csv') as csvfile:\n",
    "    titanic_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Header contains feature names\n",
    "    row = next(titanic_reader)\n",
    "    feature_names = np.array(row)\n",
    "    \n",
    "    # Load dataset, and target classes\n",
    "    titanic_X, titanic_y = [], []\n",
    "    for row in titanic_reader:  \n",
    "        titanic_X.append(row)\n",
    "        titanic_y.append(row[2]) # The target value is \"survived\"\n",
    "    \n",
    "    titanic_X = np.array(titanic_X)\n",
    "    titanic_y = np.array(titanic_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect how data looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (feature_names, titanic_X[0], titanic_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only class (1st,2nd,3rd), age (float), and sex (male, female) for our study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we keep the class, the age and the sex\n",
    "titanic_X = titanic_X[:, [1, 4, 10]]\n",
    "feature_names = feature_names[[1, 4, 10]]\n",
    "print (feature_names)\n",
    "print (titanic_X[12], titanic_y[12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some problems with missing values ('NA') for the 'age' feature. To avoid this, we will use the mean value whenever we do not have data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = titanic_X[:, 1]\n",
    "mean_age = np.mean(titanic_X[ages != 'NA', 1].astype(np.float))\n",
    "titanic_X[titanic_X[:, 1] == 'NA', 1] = mean_age\n",
    "print (feature_names)\n",
    "print (titanic_X[12], titanic_y[12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, class and sex are categorical classes, but most scikit-learn classifiers (in particular the Decision Trees we plan to use), expect real-valued attributes. We can easily convert sex  to a binary value (0=female,1=male). We will use the LabelEncoder class from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(titanic_X[:, 2])\n",
    "print (\"Categorical classes:\", label_encoder.classes_)\n",
    "integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "print (\"Integer classes:\", integer_classes)\n",
    "t = label_encoder.transform(titanic_X[:, 2])\n",
    "titanic_X[:, 2] = t\n",
    "print ('Feature names:',feature_names)\n",
    "print ('Features for instance number 12:',titanic_X[12], titanic_y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to convert the class. Since we have three different classes, we cannot convert to binary values (and using 0/1/2 values would imply an order, something we do not want). We use OneHotEncoder to get three different attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(titanic_X[:, 0])\n",
    "print (\"Categorical classes:\", label_encoder.classes_)\n",
    "integer_classes = label_encoder.transform(label_encoder.classes_).reshape(3, 1)\n",
    "print (\"Integer classes:\", integer_classes)\n",
    "enc = OneHotEncoder()\n",
    "one_hot_encoder = enc.fit(integer_classes)\n",
    "# First, convert clases to 0-(N-1) integers using label_encoder\n",
    "num_of_rows = titanic_X.shape[0]\n",
    "t = label_encoder.transform(titanic_X[:, 0]).reshape(num_of_rows, 1)\n",
    "# Second, create a sparse matrix with three columns, each one indicating if the instance belongs to the class\n",
    "new_features = one_hot_encoder.transform(t)\n",
    "# Add the new features to titanix_X\n",
    "titanic_X = np.concatenate([titanic_X, new_features.toarray()], axis = 1)\n",
    "#Eliminate converted columns\n",
    "titanic_X = np.delete(titanic_X, [0], 1)\n",
    "# Update feature names\n",
    "feature_names = ['age', 'sex', 'first_class', 'second_class', 'third_class']\n",
    "# Convert to numerical values\n",
    "titanic_X = titanic_X.astype(float)\n",
    "titanic_y = titanic_y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('New feature names:',feature_names)\n",
    "print ('Values:',titanic_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split, as usual, training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_X, titanic_y, test_size=0.25, random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a new DecisionTreeClassifier and use the fit method of the classifier to do the learning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree we have built represents a series of decisions based on the training data. To classify an instance, we should answer the question at each node. For example, at our root node, the question is: Is sex<=0.5? (are we talking about a woman?). If the answer is yes, you go to the left child node in the tree; otherwise you go to the right child node. You keep answering questions (was she in the third class?, was she in the first class?, and was she below 13 years old?), until you reach a leaf. When you are there, the prediction corresponds to the target class that has most instances (that is if the answers are given to the previous questions). In our case, if she was a woman from second class, the answer would be 1 (that is she survived), and so on. Let's drawit, using pyplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io, pydot\n",
    "dot_data = io.StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=['age','sex','1st_class','2nd_class','3rd_class'])\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "png_str = graph[0].create(format = 'png')\n",
    "\n",
    "from IPython.core.display import Image \n",
    "Image(png_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure Accuracy, precision, recall, F1 in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X,y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "    y_pred=clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print (\"Confusion matrix\")\n",
    "        print (metrics.confusion_matrix(y,y_pred),\"\\n\")\n",
    "        \n",
    "measure_performance(X_train,y_train,clf, show_classification_report=False, show_confusion_matrix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use an extreme case of cross- validation, named leave-one-out cross-validation. For each instance in the training sample, we train on the rest of the sample, and evaluate the model built on the only instance left out. After performing as many classifications as training instances, we calculate the accuracy simply as the proportion of times our method correctly predicted the class of the left-out instance, and found it is a little lower (as we expected) than the resubstitution accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, LeaveOneOut\n",
    "from scipy.stats import sem\n",
    "\n",
    "def mean_score(scores):\n",
    "    \"\"\"Print the empirical mean score and standard error of the mean.\"\"\"\n",
    "    return (\"Mean score: {0:.3f} (+/-{1:.3f})\").format(\n",
    "        np.mean(scores), sem(scores))\n",
    "\n",
    "def loo_cv(X_train,y_train,clf):\n",
    "    # Perform Leave-One-Out cross validation\n",
    "    # We are preforming 1313 classifications!\n",
    "    loo = LeaveOneOut(X_train[:].shape[0])\n",
    "    scores=np.zeros(X_train[:].shape[0])\n",
    "    for train_index,test_index in loo:\n",
    "        X_train_cv, X_test_cv= X_train[train_index], X_train[test_index]\n",
    "        y_train_cv, y_test_cv= y_train[train_index], y_train[test_index]\n",
    "        clf = clf.fit(X_train_cv,y_train_cv)\n",
    "        y_pred=clf.predict(X_test_cv)\n",
    "        scores[test_index]=metrics.accuracy_score(y_test_cv.astype(int), y_pred.astype(int))\n",
    "    print (mean_score(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loo_cv(X_train, y_train,clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common criticism to decision trees is that once the training set is divided after answering a question, it is not possible to reconsider this decision. For example, if we divide men and women, every subsequent question would be only about men or women, and the method could not consider another type of question (say, age less than a year, irrespective of the gender). Random Forests try to introduce some level of randomization in each step, proposing alternative trees and combining them to get the final prediction. These types of algorithms that consider several classifiers answering the same question are called ensemble methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10,random_state=33)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "loo_cv(X_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate performance on future data, evaluate on the training set and test on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_dt=tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
    "clf_dt.fit(X_train,y_train)\n",
    "measure_performance(X_test,y_test,clf_dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
